### run.py 
* 主程序
  * 创建基础运行期runner后，AMP算法注册有四个组件：
    * AMPAgent: 训练时的算法逻辑，实现AMP的核心训练循环
    * AMPPlayerContinuous: 推理时的播放器，用于执行训练好的策略
    * ModelAMPContinuous: 神经网络模型包装器，定义网络的前向传播
    * AMPBuilder: 网络架构构建器，定义网络的具体结构
  * ASE算法注册（与AMP注册过程相同）：
    * 基于AMP扩展，增加了技能嵌入学习
    * 能够学习多样化的运动技能
    * 支持技能的组合和转换
  * HRL分层强化学习算法注册：
    * 实现分层强化学习
    * 包含高层控制器（HLC）和低层控制器（LLC）
    * 适合复杂的长期任务规划
  * 训练阶段流程：
    * 配置选择算法（算法工厂）
    * 创建Agent（训练agent）
    * 环境交互（环境采样）
    * 策略更新（网络训练）
    * 模型保存
  * 推理阶段流程：
    * 加载模型（模型工厂）
    * 创建Player（播放器工厂）
    * 执行策略（网络推理）
    * 输出动作（动作执行）

* create_rlgpu_env：
  * 设备管理——处理GPU/CPU之间的数据传输；
  * 数值剪裁——限制观察值与动作值的范围；
  * Isaac Gym适配——将Isaac Gym的原始接口转换为更标准的格式

* RLGPUEnv：外层包装器
  * rl_games接口适配：实现rl_games期望的标准接口
  * 状态字典管理：将观察和状态组织成字典格式
  * 元数据提供：提供环境信息给rl_games框架
  * 接收VecTaskPythonWrapper作为env,并将step方法委托给接收VecTaskPythonWrapper
  * 运行时数据流：  
    rl_games调用（神经网络输出action） → RLGPUEnv.step()（委托给VecTaskPythonWrapper，计算下一步状态、奖励） → VecTaskPythonWrapper.step()（委托给Isaac Gym任务）  
    数据回传：Isaac Gym → VecTaskPythonWrapper → RLGPUEnv → rl_games

### torch_runner.py
* 根据配置文件中指定的算法名称创建对应的智能体
* While循环：支持批量实验和超参数搜索
* 实验管理：自动化多组实验的执行和结果记录

### a2c_common.py
* ase算法所继承的基类A2CBase，可进行PBT(Population Based Training，基于种群的训练)，这是一种自动化超参数优化的训练方法，可同时训练多个智能体（形成一个种群），每个智能体使用不同的超参数配置，训练过程中表现差的智能体会复制表现好的智能体的数数，并对超参数进行变异，继续探索  
可自适应超参数调优：无需预先设定最优超参数；并行探索：同时测试多种参数组合；动态优化：训练过程中持续改进超参数

### humanoid.py
* 初始化：
  * 为不同身体部位设置终止高度阈值，用于判断人形机器人是否摔倒，当机器人的特定身体部位低于相应阈值时，训练episode将被终止（判定为摔倒失败）
  * 根据不同的人形机器人模型配置相应的物理和观察参数（自由度、观测空间维度、动作空间维度、身体部位映射关系）。其中有身体可动部位id,自由度偏移量（每个关节在DOF数组中的起始位置，因为若是球关节，可能一个关节在DOF数组中占3位，而铰链关节仅占1位），观察维度（根部高度，身体部位的3D位置、6D旋转表示、3D线速度、3D角速度）。剑盾模型额外加了两个身体部位
  * 获取Gym中的状态张量
* _create_envs：
  * 加载人形机器人资产文件
  * 要根据脚的名称进行修改，这里要获取左右脚的索引id，以在脚上添加力传感器
  * 为每个环境创建Actor实例
* _build_env：
  * 负责单个Actor创建
  * 设置Actor的初始位置和姿态start_pose,注意这里要修改 **基座高度**
  * 设置外观颜色
  * 设置控制模式
* _reset_actors：
  * 重置actor时，设置初始关节位置与速度，代码中设置为0,但实际人物应该需要修改

### humanoid_amp.py
* 初始化：
  * 加载参考动作

### humanoid_amp_getup.py
* 初始化：
  * 生成预计算的跌倒状态库，为人形机器人的恢复训练提供真实的跌倒姿态
* _generate_fall_states：
  * 首先置生成跌倒状态的模拟步数。150步的物理模拟时间足够让人形机器人跌倒并稳定在倒地姿态
  * 随机化人形机器人的方向，创建随机的起始方向，会导致不同的跌倒模式
  * 应用随机动作来创建混乱运动，这会使人形机器人失去平衡并开始跌倒
  * 运行150步物理模拟，随机动作 + 随机方向导致人形机器人以各种方式跌倒，到最后，人形机器人应该处于倒塌/跌倒状态
  * 存储跌倒的根状态供以后使用，并将速度状态设置为0，使跌倒状态处于静止状态（不翻滚）